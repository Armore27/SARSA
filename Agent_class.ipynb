{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# For animation \n",
    "from IPython.display import clear_output\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, method, start_alpha = 0.3, start_gamma = 0.9, start_epsilon = 0.5):\n",
    "        \"\"\"method: one of 'q_learning', 'sarsa' or 'expected_sarsa' \"\"\"\n",
    "        self.method = method\n",
    "        self.env = gym.make('Taxi-v2')\n",
    "        self.n_squares = 25 \n",
    "        self.n_passenger_locs = 5 \n",
    "        self.n_dropoffs = 4 \n",
    "        self.n_actions = self.env.action_space.n\n",
    "        self.epsilon = start_epsilon\n",
    "        self.gamma = start_gamma\n",
    "        self.alpha = start_alpha\n",
    "        # Set up initial q-table \n",
    "        self.q = np.zeros(shape = (self.n_squares*self.n_passenger_locs*self.n_dropoffs, self.env.action_space.n))\n",
    "        # Set up policy pi, init as equiprobable random policy\n",
    "        self.pi = np.zeros_like(self.q)\n",
    "        for i in range(self.pi.shape[0]): \n",
    "            for a in range(self.n_actions): \n",
    "                self.pi[i,a] = 1/self.n_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def simulate_episode(self):\n",
    "        s = self.env.reset()\n",
    "        done = False\n",
    "        r_sum = 0 \n",
    "        n_steps = 0 \n",
    "        gam = self.gamma\n",
    "        while not done: \n",
    "            n_steps += 1\n",
    "            # take action from policy\n",
    "            x = np.random.random()\n",
    "            a = np.argmax(np.cumsum(self.pi[s,:]) > x) \n",
    "            # take step \n",
    "            s_prime,r,done,info = self.env.step(a)    \n",
    "            if self.method == 'q_learning': \n",
    "                a_prime = np.random.choice(np.where(self.q[s_prime] == max(self.q[s_prime]))[0])\n",
    "                self.q[s,a] = self.q[s,a] + self.alpha * \\\n",
    "                    (r + gam*self.q[s_prime,a_prime] - self.q[s,a])\n",
    "            elif self.method == 'sarsa': \n",
    "                a_prime = np.argmax(np.cumsum(self.pi[s_prime,:]) > np.random.random())\n",
    "                self.q[s,a] = self.q[s,a] + self.alpha * \\\n",
    "                    (r + gam*self.q[s_prime,a_prime ] - self.q[s,a])\n",
    "            elif self.method == 'expected_sarsa':\n",
    "                self.q[s,a] = self.q[s,a] + self.alpha * \\\n",
    "                    (r + gam* np.dot(self.pi[s_prime,:],self.q[s_prime,:]) - self.q[s,a])\n",
    "            else: \n",
    "                raise Exception(\"Invalid method provided\")\n",
    "            # update policy\n",
    "            best_a = np.random.choice(np.where(self.q[s] == max(self.q[s]))[0])\n",
    "            for i in range(self.n_actions): \n",
    "                if i == best_a:      self.pi[s,i] = 1 - (self.n_actions-1)*(self.epsilon / self.n_actions)\n",
    "                else:                self.pi[s,i] = self.epsilon / self.n_actions\n",
    "\n",
    "            # decay gamma close to the end of the episode\n",
    "            if n_steps > 185: \n",
    "                gam *= 0.875\n",
    "            s = s_prime\n",
    "            r_sum += r\n",
    "        return r_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_a = np.random.choice(np.where(self.q[s] == max(self.q[s]))[0])\n",
    "for i in range(self.n_actions): \n",
    "    if i == best_a:      self.pi[s,i] = 1 - (self.n_actions-1)*(self.epsilon / self.n_actions)\n",
    "    else:                self.pi[s,i] = self.epsilon / self.n_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(agent, n_episodes= 200001, epsilon_decay = 0.99995, alpha_decay = 0.99995, print_trace = False):\n",
    "    r_sums = []\n",
    "    for ep in range(n_episodes): \n",
    "        r_sum = agent.simulate_episode()\n",
    "        # decrease epsilon and learning rate \n",
    "        agent.epsilon *= epsilon_decay\n",
    "        agent.alpha *= alpha_decay\n",
    "        if print_trace: \n",
    "            if ep % 20000 == 0 and ep > 0 : \n",
    "                print(\"Episode:\", ep, \"alpha:\", np.round(agent.alpha, 3), \"epsilon:\",  np.round(agent.epsilon, 3))\n",
    "                print (\"Last 100 episodes avg reward: \", np.mean(r_sums[ep-100:ep]))\n",
    "        r_sums.append(r_sum)\n",
    "    return r_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create agents \n",
    "sarsa_agent = Agent(method='sarsa')\n",
    "e_sarsa_agent = Agent(method='expected_sarsa')\n",
    "q_learning_agent = Agent(method='q_learning')\n",
    "\n",
    "# Train agents\n",
    "r_sums_sarsa = train_agent(sarsa_agent, print_trace=True)\n",
    "r_sums_e_sarsa = train_agent(e_sarsa_agent, print_trace=True)\n",
    "r_sums_q_learning = train_agent(q_learning_agent, print_trace=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
