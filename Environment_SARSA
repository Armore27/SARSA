import sys
from contextlib import closing
from six import StringIO
from gym import utils
from gym.envs.toy_text import discrete
import numpy as np
import matlab.engine

class MirrorENV(discrete.DiscreteEnv):
    """
    Description:
    There is a mirror, which is moved by 5 motors. Each motor moves mirror into the specific direction. The main task is to find 
    most suitable mirror position according to the Zernike polinomials. 
    The end of the episode is when the mirror in the most suitable position.
    Observations: 
    There are ??? discrete states since there are ??? mirror positions.
        
    Actions:
    There are 45 discrete deterministic actions: (Imported from MATLAB)
    - 0: sdg1(sg1+a)
    - 1: sdv1(sv1+a)
    - 2: os1(z1+a)
    - 3: gora1(g1+a) 
    - 4: vera1(v1+a)
    - 5: sdg1(sg1-a)
    - 6: sdv1(sv1-a)
    - 7: os1(z1-a)
    - 8: gora1(g1-a)
    - 9: vera1(v1-a)
    - 10: gora1(g1+a)sdg1(sg1+a)
    - 11: vera1(v1+a)sdv1(sv1+a)
    - 12: os1(z1+a)sdg1(sg1+a)
    - 13: os1(z1+a)sdv1(sv1+a)
    - 14: gora1(g1+a)sdg1(sg1-a)
    - 15: vera1(v1+a)sdv1(sv1-a)
    - 16: os1(z1+a)sdg1(sg1-a)
    - 17: os1(z1+a)sdv1(sv1-a)
    - 18: gora1(g1-a)sdg1(sg1-a)
    - 19: vera1(v1-a)sdv1(sv1-a)
    - 20: os1(z1-a)sdg1(sg1-a)
    - 21: os1(z1-a)sdv1(sv1-a)
    - 22: gora1(g1-a)sdg1(sg1+a)
    - 23: vera1(v1-a)sdv1(sv1+a)
    - 24: os1(z1-a)sdg1(sg1+a)
    - 25: os1(z1-a)sdv1(sv1+a)
    - 26: sdg1(sg1+a)sdv1(sv1+a)
    - 27: sdg1(sg1-a)sdv1(sv1-a)
    - 28: sdg1(sg1+a)sdv1(sv1-a)
    - 29: sdv1(sv1+a)sdg1(sg1-a)
    - 30: gora1(g1+a)sdv1(sv1+a)
    - 31: vera1(v1+a)sdg1(sg1+a)
    - 32: os1(z1+a)gora1(g1+a)
    - 33: os1(z1+a)vera1(v1+a)
    - 34: gora1(g1+a)sdv1(sv1-a)
    - 35: vera1(v1+a)sdg1(sg1-a)
    - 36: os1(z1+a)gora1(g1-a)
    - 37: os1(z1+a)vera1(v1-a)
    - 38: gora1(g1-a)sdv1(sv1+a)
    - 39: vera1(v1-a)sdg1(sg1+a)
    - 40: os1(z1-a)gora1(g1+a)
    - 41: os1(z1-a)vera1(v1+a)
    - 42: gora1(g1-a)sdv1(sv1-a)
    - 43: vera1(v1-a)sdg1(sg1-a)
    - 44: os1(z1-a)gora1(g1-a)
    - 45: os1(z1-a)vera1(v1-a)
    
    Rewards: 
    Reward of -1 for each possible action and an additional reward for completed system +100.
    Reward of -10 for each unnecessary action.
    
    State space is represented by:
    14 Zernike polinomials (an array) (Imported from MATLAB)
        
    """
    metadata = {'render.modes': ['human', 'ansi']}
    
   def __init__(self):
        
        eng = matlab.engine.start_matlab()             #для работы со средой MATLAB
        
        coef_hands = coef_hands.mat                    #импортировать таблицу значений из MATLAB в качестве массива
        num_states = 500                               #посчитать количество возможных состояний системы с учетом размеров матрицы и размеров изображения
        initial_state_distrib = np.zeros(num_states)   #не понятно, надо ли оно
        num_actions = 45                               #количество всех возможных действий 
        state = eng.Obrabotka()                        #состояние задается 14 полиномами Цернике
        P = {state: {action: []
                     for action in range(num_actions)} for state in range(num_states)}
        for action in range(num_actions):
            state
            reward = -1                                #награда по умолчанию, когда не совершено ни одно действие
            done = False
            a = 1        
            #возможные действия, награда = -1
            if action == 0:
                b = sg1+a
                eng.sdg1(b)
                reward = -1
            if action == 1:
                eng.sdv1(sv1+a)
                reward = -1
            if action == 2:
                eng.os1(z1+a)
                reward = -1
            if action == 3:
                eng.gora1(g1+a)
                reward = -1
            if action == 4:
                eng.vera1(v1+a)
                reward = -1
            if action == 5:
                eng.sdg1(sg1-a)
                reward = -1
            if action == 6:
                eng.sdv1(sv1-a)
                reward = -1
            if action == 7:
                eng.os1(z1-a)
                reward = -1
            if action == 8:
                eng.gora1(g1-a)
                reward = -1
            if action == 9:
                eng.vera1(v1-a)
                reward = -1
            if action == 10:
                eng.gora1(g1+a)
                eng.sdg1(sg1+a)
                reward = -1
            if action == 11:
                eng.vera1(v1+a)
                eng.sdv1(sv1+a)
                reward = -1
            if action == 12:
                eng.os1(z1+a)
                eng.sdg1(sg1+a)
                reward = -1
            if action == 13:
                eng.os1(z1+a)
                eng.sdv1(sv1+a)
                reward = -1
            if action == 14:
                eng.gora1(g1+a)
                eng.sdg1(sg1-a)
                reward = -1
            if action == 15:
                eng.vera1(v1+a)
                eng.sdv1(sv1-a)
                reward = -1
            if action == 16:
                eng.os1(z1+a)
                eng.sdg1(sg1-a)
                reward = -1
            if action == 17:
                eng.os1(z1+a)
                eng.sdv1(sv1-a)
                reward = -1
            if action == 18:
                eng.gora1(g1-a)
                eng.sdg1(sg1-a)
                reward = -1
            if action == 19:
                eng.vera1(v1-a)
                eng.sdv1(sv1-a)
                reward = -1
            if action == 20:
                eng.os1(z1-a)
                eng.sdg1(sg1-a)
                reward = -1
            if action == 21:
                eng.os1(z1-a)
                eng.sdv1(sv1-a)
                reward = -1
            if action == 22:
                eng.gora1(g1-a)
                eng.sdg1(sg1+a)
                reward = -1
            if action == 23:
                eng.era1(v1-a)
                eng.sdv1(sv1+a)
                reward = -1
            if action == 24:
                eng.os1(z1-a)
                eng.sdg1(sg1+a)
                reward = -1
            if action == 25:
                eng.os1(z1-a)
                eng.sdv1(sv1+a)
                reward = -1
            if action == 26:
                eng.sdg1(sg1+a)
                eng.sdv1(sv1+a)
                reward = -1
            if action == 27:
                eng.sdg1(sg1-a)
                eng.sdv1(sv1-a)
                reward = -1
            if action == 28:
                eng.sdg1(sg1+a)
                eng.sdv1(sv1-a)
                reward = -1
            if action == 29:
                eng.sdv1(sv1+a)
                eng.sdg1(sg1-a)
                reward = -1
                    
            #бесполезные действия, награда = -10
            if action == 30:
                eng.gora1(g1+a)
                eng.sdv1(sv1+a)
                reward = -10
            if action == 31:
                eng.vera1(v1+a)
                eng.sdg1(sg1+a)
                reward = -10
            if action == 32:
                eng.os1(z1+a)
                eng.gora1(g1+a)
                reward = -10
            if action == 33:
                eng.os1(z1+a)
                eng.vera1(v1+a)
                reward = -10
            if action == 34:
                eng.gora1(g1+a)
                eng.sdv1(sv1-a)
                reward = -10
            if action == 35:
                eng.vera1(v1+a)
                eng.sdg1(sg1-a)
                reward = -10
            if action == 36:
                eng.os1(z1+a)
                eng.gora1(g1-a)
                reward = -10
            if action == 37:
                eng.os1(z1+a)
                eng.vera1(v1-a)
                reward = -10
            if action == 38:
                eng.gora1(g1-a)
                eng.sdv1(sv1+a)
                reward = -10
            if action == 39:
                eng.vera1(v1-a)
                eng.sdg1(sg1+a)
                reward = -10
            if action == 40:
                eng.os1(z1-a)
                eng.gora1(g1+a)
                reward = -10
            if action == 41:
                eng.os1(z1-a)
                eng.vera1(v1+a)
                reward = -10
            if action == 42:
                eng.gora1(g1-a)
                eng.sdv1(sv1-a)
                reward = -10
            if action == 43:
                eng.vera1(v1-a)
                eng.sdg1(sg1-a)
                reward = -10
            if action == 44:
                eng.os1(z1-a)
                eng.gora1(g1-a)
                reward = -10
            if action == 45:
                eng.os1(z1-a)
                eng.vera1(v1-a)
                reward = -10
                
            new_state = eng.Obrabotka()                      #получение нового состояния после проделанного действия
               
            if new_state <= coef_hands                       #проверка, не является ли состояние наилучшим
                reward = 100
                    
            P[state][action].append((1.0, new_state, reward, done))
                
        discrete.DiscreteEnv.__init__(self, num_states, num_actions, P)


    def render(self, mode='human'):
        outfile = StringIO() if mode == 'ansi' else sys.stdout

        out = self.desc.copy().tolist()
        out = [[c.decode('utf-8') for c in line] for line in out]
        taxi_row, taxi_col, pass_idx, dest_idx = self.decode(self.s)

        def ul(x): return "_" if x == " " else x
        if pass_idx < 4:
            out[1 + taxi_row][2 * taxi_col + 1] = utils.colorize(
                out[1 + taxi_row][2 * taxi_col + 1], 'yellow', highlight=True)
            pi, pj = self.locs[pass_idx]
            out[1 + pi][2 * pj + 1] = utils.colorize(out[1 + pi][2 * pj + 1], 'blue', bold=True)
        else:  # passenger in taxi
            out[1 + taxi_row][2 * taxi_col + 1] = utils.colorize(
                ul(out[1 + taxi_row][2 * taxi_col + 1]), 'green', highlight=True)

        di, dj = self.locs[dest_idx]
        out[1 + di][2 * dj + 1] = utils.colorize(out[1 + di][2 * dj + 1], 'magenta')
        outfile.write("\n".join(["".join(row) for row in out]) + "\n")
        if self.lastaction is not None:
            outfile.write("  ({})\n".format(["South", "North", "East", "West", "Pickup", "Dropoff"][self.lastaction]))
        else: outfile.write("\n")

        # No need to return anything for human
        if mode != 'human':
            with closing(outfile):
                return outfile.getvalue()
